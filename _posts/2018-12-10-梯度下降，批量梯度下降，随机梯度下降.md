梯度下降：使用所有的样本进行训练更新权重参数。可以获取全局最优解，可保证每一次更新权值，降低损失函数，可以并行实现；当样本数目比较多时，训练过程慢。

批量梯度下降：使用小批量样本进行训练更新权值参数。

随机梯度下降：使用一个样本来进行更新，更新频繁，训练速度快；准确度下降，不易并行实现。